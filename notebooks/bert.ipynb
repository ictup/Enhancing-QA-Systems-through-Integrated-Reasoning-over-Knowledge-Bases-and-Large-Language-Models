{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a21e888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERTScore model (this may take a moment)...\n",
      "BERTScore model loaded.\n",
      "Reading input file: D:\\liulanqi1\\MindMap-main\\MindMap-main\\output8_chatgpt_cols_with_llama_and_deepseek.csv\n",
      "Successfully read CSV file, found 29 rows.\n",
      "Input columns: ['Question', 'Label', 'Zero_shot_GPT4o_mini', 'Zero_shot_GPT4o', 'One_shot_GPT4o_mini', 'One_shot_GPT4o', 'Few_shot_GPT4o_mini', 'Few_shot_GPT4o', 'CoT_GPT4o_mini', 'CoT_GPT4o', 'llama3_pred', 'DeepSeek_Pred']\n",
      "Warning: The following candidate columns were not found and will be skipped: ['Mindmap', 'BM25_retrieval', 'KG_retrieval', 'KG_retrieval_multipath', 'KG_self-consistency', 'Zero_shot_GPT4o_mini_kg', 'zero_shot_gpt4_kg', 'AutoGen_Single_Result', 'AutoGen_Single_Result_nokg', 'AutoGen_Multi_Result', 'Multi-Agent Result_nokg', 'KG_ToT_Sampling_Comprehensive_BFS', 'KG_ToT_Sampling_Comprehensive_DFS', 'KG_ToT_Sampling_Comparative_BFS', 'KG_ToT_Sampling_Comparative_DFS', 'KG_ToT_Sequential_Comprehensive_BFS', 'KG_ToT_Sequential_Comprehensive_DFS', 'KG_ToT_Sequential_Comparative_BFS', 'KG_ToT_Sequential_Comparative_DFS', 'Mindmap_nokg', 'Zero_shot_GPT4o_mini_kg', 'zero_shot_gpt4_kg', 'AutoGen_Single_Result_nokg', 'Multi-Agent Result_nokg']\n",
      "Initialized new columns in DataFrame for BERTscore results.\n",
      "\n",
      "Processing row 1/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 2/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 3/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 4/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 5/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 6/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 7/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 8/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 9/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 10/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 11/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 12/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 13/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 14/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 15/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 16/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 17/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 18/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 19/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 20/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 21/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 22/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 23/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 24/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 25/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 26/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 27/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 28/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 29/29...\n",
      "  Using BERTScore model: distilbert-base-uncased\n",
      "  Using BERTScore model: xlm-roberta-large\n",
      "  Using BERTScore model: roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results to: D:\\liulanqi1\\MindMap-main\\MindMap-main\\output_bert_score_multi_models_revised.csv\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\output8_chatgpt_cols_with_llama_and_deepseek.csv'\n",
    "OUTPUT_FILE = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\output_bert_score_multi_models_revised.csv' # Changed output filename\n",
    "\n",
    "# Reference column name in INPUT_FILE\n",
    "REFERENCE_COLUMN_NAME = \"Label\"\n",
    "\n",
    "# Candidate output column names from your INPUT_FILE\n",
    "CANDIDATE_COLUMN_NAMES = ['Mindmap', 'BM25_retrieval', 'KG_retrieval', 'KG_retrieval_multipath', 'KG_self-consistency', 'Zero_shot_GPT4o_mini_kg', 'zero_shot_gpt4_kg', 'Zero_shot_GPT4o_mini', 'Zero_shot_GPT4o', 'One_shot_GPT4o_mini', 'One_shot_GPT4o', 'Few_shot_GPT4o_mini', 'Few_shot_GPT4o', 'CoT_GPT4o_mini', 'CoT_GPT4o', 'AutoGen_Single_Result', 'AutoGen_Single_Result_nokg', 'AutoGen_Multi_Result', 'Multi-Agent Result_nokg', 'KG_ToT_Sampling_Comprehensive_BFS', 'KG_ToT_Sampling_Comprehensive_DFS', 'KG_ToT_Sampling_Comparative_BFS', 'KG_ToT_Sampling_Comparative_DFS', 'KG_ToT_Sequential_Comprehensive_BFS', 'KG_ToT_Sequential_Comprehensive_DFS', 'KG_ToT_Sequential_Comparative_BFS', 'KG_ToT_Sequential_Comparative_DFS','Mindmap_nokg','Zero_shot_GPT4o_mini_kg','zero_shot_gpt4_kg','AutoGen_Single_Result_nokg','Multi-Agent Result_nokg'\n",
    "]\n",
    "\n",
    "\n",
    "# BERTScore models to use\n",
    "MODELS_TO_USE = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"xlm-roberta-large\",\n",
    "    \"roberta-large\",\n",
    "]\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def clean_text_for_csv(text):\n",
    "    \"\"\"Clean text: convert to string and replace newlines with spaces.\"\"\"\n",
    "    if pd.isna(text): # Handle NaN values explicitly\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "# --- Main Script ---\n",
    "try:\n",
    "    # Load BERTScore evaluator\n",
    "    print(\"Loading BERTScore model (this may take a moment)...\")\n",
    "    bertscore_evaluator = load(\"bertscore\")\n",
    "    print(\"BERTScore model loaded.\")\n",
    "\n",
    "    # Read input CSV\n",
    "    print(f\"Reading input file: {INPUT_FILE}\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Successfully read CSV file, found {len(df)} rows.\")\n",
    "    print(f\"Input columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Verify reference column exists\n",
    "    if REFERENCE_COLUMN_NAME not in df.columns:\n",
    "        raise ValueError(f\"Reference column '{REFERENCE_COLUMN_NAME}' not found in the input CSV.\")\n",
    "\n",
    "    # Verify all candidate columns exist\n",
    "    missing_candidates = [col for col in CANDIDATE_COLUMN_NAMES if col not in df.columns]\n",
    "    if missing_candidates:\n",
    "        print(f\"Warning: The following candidate columns were not found and will be skipped: {missing_candidates}\")\n",
    "        # Filter out missing columns from processing list\n",
    "        CANDIDATE_COLUMN_NAMES = [col for col in CANDIDATE_COLUMN_NAMES if col in df.columns]\n",
    "\n",
    "    if not CANDIDATE_COLUMN_NAMES:\n",
    "        raise ValueError(\"No valid candidate columns found to process after checking input CSV.\")\n",
    "\n",
    "\n",
    "    # Initialize new columns for scores and averages in the DataFrame\n",
    "    for model_full_name in MODELS_TO_USE:\n",
    "        model_short_name = model_full_name.split(\"-\")[0] # e.g., \"distilbert\", \"xlm\", \"roberta\"\n",
    "        # Ensure short names are unique if first parts are same (e.g. xlm-roberta-large vs xlm-roberta-base)\n",
    "        # For this set, they are unique. If not, adjust model_short_name logic.\n",
    "\n",
    "        for candidate_name in CANDIDATE_COLUMN_NAMES:\n",
    "            # Sanitize candidate_name if it contains characters not suitable for column names,\n",
    "            # though the provided ones seem fine.\n",
    "            sanitized_candidate_name = candidate_name.replace(\" \", \"_\").replace(\"-\", \"_\") # Basic sanitization\n",
    "\n",
    "            df[f\"{model_short_name}_{sanitized_candidate_name}_precision\"] = np.nan\n",
    "            df[f\"{model_short_name}_{sanitized_candidate_name}_recall\"] = np.nan\n",
    "            df[f\"{model_short_name}_{sanitized_candidate_name}_f1\"] = np.nan\n",
    "\n",
    "        # Columns for per-model average scores (average over all candidates for that row)\n",
    "        df[f\"{model_short_name}_avg_precision\"] = np.nan\n",
    "        df[f\"{model_short_name}_avg_recall\"] = np.nan\n",
    "        df[f\"{model_short_name}_avg_f1\"] = np.nan\n",
    "    print(\"Initialized new columns in DataFrame for BERTscore results.\")\n",
    "\n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\nProcessing row {idx + 1}/{len(df)}...\")\n",
    "        try:\n",
    "            reference_text_cleaned = clean_text_for_csv(row[REFERENCE_COLUMN_NAME])\n",
    "            if not reference_text_cleaned.strip():\n",
    "                print(f\"  Warning: Empty or whitespace-only reference text in row {idx + 1}. Skipping BERTscore for this row.\")\n",
    "                continue\n",
    "            references_list = [reference_text_cleaned]\n",
    "\n",
    "            for model_full_name in MODELS_TO_USE:\n",
    "                model_short_name = model_full_name.split(\"-\")[0]\n",
    "                print(f\"  Using BERTScore model: {model_full_name}\")\n",
    "\n",
    "                current_model_precisions = []\n",
    "                current_model_recalls = []\n",
    "                current_model_f1s = []\n",
    "\n",
    "                for candidate_name in CANDIDATE_COLUMN_NAMES:\n",
    "                    sanitized_candidate_name = candidate_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "                    prediction_text_cleaned = clean_text_for_csv(row[candidate_name])\n",
    "\n",
    "                    if not prediction_text_cleaned.strip():\n",
    "                        print(f\"    Warning: Empty or whitespace-only prediction text for '{candidate_name}' in row {idx + 1}. Assigning NaN for this candidate.\")\n",
    "                        # Scores will remain NaN as initialized\n",
    "                        continue\n",
    "\n",
    "                    predictions_list = [prediction_text_cleaned]\n",
    "\n",
    "                    try:\n",
    "                        results = bertscore_evaluator.compute(\n",
    "                            predictions=predictions_list,\n",
    "                            references=references_list,\n",
    "                            model_type=model_full_name, # Use the full model name for bertscore compute\n",
    "                            # lang=\"en\" # Specify language if not English or for multilingual models\n",
    "                        )\n",
    "                        precision = results[\"precision\"][0]\n",
    "                        recall = results[\"recall\"][0]\n",
    "                        f1 = results[\"f1\"][0]\n",
    "\n",
    "                        df.at[idx, f\"{model_short_name}_{sanitized_candidate_name}_precision\"] = precision\n",
    "                        df.at[idx, f\"{model_short_name}_{sanitized_candidate_name}_recall\"] = recall\n",
    "                        df.at[idx, f\"{model_short_name}_{sanitized_candidate_name}_f1\"] = f1\n",
    "\n",
    "                        current_model_precisions.append(precision)\n",
    "                        current_model_recalls.append(recall)\n",
    "                        current_model_f1s.append(f1)\n",
    "                        # print(f\"    {candidate_name} - P: {precision:.4f}, R: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "                    except Exception as e_bert:\n",
    "                        print(f\"    Error computing BERTScore for '{candidate_name}' with model '{model_full_name}': {e_bert}\")\n",
    "                        # Scores will remain NaN\n",
    "\n",
    "                # Calculate and store averages for the current model for this row\n",
    "                if current_model_f1s: # Check if any scores were successfully computed\n",
    "                    df.at[idx, f\"{model_short_name}_avg_precision\"] = np.mean(current_model_precisions)\n",
    "                    df.at[idx, f\"{model_short_name}_avg_recall\"] = np.mean(current_model_recalls)\n",
    "                    df.at[idx, f\"{model_short_name}_avg_f1\"] = np.mean(current_model_f1s)\n",
    "                    # print(f\"  Model {model_short_name} Avg - P: {np.mean(current_model_precisions):.4f}, R: {np.mean(current_model_recalls):.4f}, F1: {np.mean(current_model_f1s):.4f}\")\n",
    "                else:\n",
    "                    print(f\"  No valid BERTscores computed for model {model_short_name} in this row to average.\")\n",
    "        except KeyError as e_key:\n",
    "            print(f\"  Skipping row {idx+1} due to missing key: {e_key}. Ensure all specified columns exist.\")\n",
    "        except Exception as e_row:\n",
    "            print(f\"  An unexpected error occurred while processing row {idx + 1}: {e_row}\")\n",
    "\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    print(f\"\\nSaving results to: {OUTPUT_FILE}\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL) # utf-8-sig for Excel compatibility\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{INPUT_FILE}' not found.\")\n",
    "except ValueError as ve:\n",
    "    print(f\"Configuration Error: {ve}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775b376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read aggregated scores CSV: D:\\liulanqi1\\MindMap-main\\MindMap-main\\output_bert_score_multi_models_revised.csv, found 29 rows.\n",
      "Detected/Using model prefixes: ['distilbert', 'roberta', 'xlm']\n",
      "\n",
      "Candidate method average scores saved to D:\\liulanqi1\\MindMap-main\\MindMap-main\\bertscore_aggregates_and_reports_revised.csv\n",
      "\n",
      "--- Table 1: Avg BERTScore per Candidate Method (across BERTScore Models) ---\n",
      "Sorted by F1 score (descending)\n",
      "====================================================================================================\n",
      "Candidate Method                              |  Avg Precision  |   Avg Recall    |     Avg F1     \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Few-shot GPT4o                                |     0.7844      |     0.8287      |     0.8058     \n",
      "One-shot GPT4o                                |     0.7652      |     0.8323      |     0.7972     \n",
      "Few-shot GPT4o-mini                           |     0.7580      |     0.8168      |     0.7861     \n",
      "One-shot GPT4o-mini                           |     0.7491      |     0.8195      |     0.7826     \n",
      "Zero-shot GPT4o                               |     0.7327      |     0.8259      |     0.7761     \n",
      "CoT GPT4o                                     |     0.7362      |     0.8167      |     0.7739     \n",
      "Zero-shot GPT4o-mini                          |     0.7224      |     0.8165      |     0.7660     \n",
      "CoT GPT4o-mini                                |     0.7242      |     0.8071      |     0.7629     \n",
      "---------------------------------------------------------------------------------------------------\n",
      "OVERALL (Avg of above)                        |     0.7465      |     0.8204      |     0.7813     \n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- Table 2: Avg BERTScore per BERTScore Model (across Candidate Methods) ---\n",
      "Sorted by F1 score (descending)\n",
      "==========================================================================================\n",
      "BERTScore Model                |  Avg Precision  |   Avg Recall    |     Avg F1     \n",
      "------------------------------------------------------------------------------------\n",
      "xlm                            |     0.8054      |     0.8576      |     0.8306     \n",
      "roberta                        |     0.7621      |     0.8372      |     0.7978     \n",
      "distilbert                     |     0.6720      |     0.7665      |     0.7155     \n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "--- Table 3: Detailed Avg BERTScore (BERTScore Model vs. Candidate Method) ---\n",
      "Sorted by F1 score (descending)\n",
      "========================================================================================================================\n",
      "BERTScore Model                | Candidate Method                              |  Avg Precision  |   Avg Recall    |     Avg F1     \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "xlm                            | Few-shot GPT4o                                |     0.8270      |     0.8639      |     0.8450     \n",
      "xlm                            | One-shot GPT4o                                |     0.8188      |     0.8677      |     0.8425     \n",
      "xlm                            | Few-shot GPT4o-mini                           |     0.8103      |     0.8562      |     0.8326     \n",
      "xlm                            | One-shot GPT4o-mini                           |     0.8036      |     0.8577      |     0.8298     \n",
      "xlm                            | Zero-shot GPT4o                               |     0.8005      |     0.8613      |     0.8297     \n",
      "xlm                            | CoT GPT4o                                     |     0.7987      |     0.8528      |     0.8248     \n",
      "xlm                            | Zero-shot GPT4o-mini                          |     0.7943      |     0.8551      |     0.8236     \n",
      "roberta                        | Few-shot GPT4o                                |     0.7964      |     0.8464      |     0.8206     \n",
      "xlm                            | CoT GPT4o-mini                                |     0.7899      |     0.8460      |     0.8170     \n",
      "roberta                        | One-shot GPT4o                                |     0.7697      |     0.8477      |     0.8068     \n",
      "roberta                        | CoT GPT4o                                     |     0.7663      |     0.8347      |     0.7990     \n",
      "roberta                        | Few-shot GPT4o-mini                           |     0.7637      |     0.8368      |     0.7985     \n",
      "roberta                        | One-shot GPT4o-mini                           |     0.7555      |     0.8377      |     0.7944     \n",
      "roberta                        | Zero-shot GPT4o                               |     0.7511      |     0.8393      |     0.7927     \n",
      "roberta                        | CoT GPT4o-mini                                |     0.7520      |     0.8225      |     0.7856     \n",
      "roberta                        | Zero-shot GPT4o-mini                          |     0.7423      |     0.8328      |     0.7849     \n",
      "distilbert                     | Few-shot GPT4o                                |     0.7298      |     0.7759      |     0.7518     \n",
      "distilbert                     | One-shot GPT4o                                |     0.7072      |     0.7813      |     0.7423     \n",
      "distilbert                     | Few-shot GPT4o-mini                           |     0.6999      |     0.7575      |     0.7274     \n",
      "distilbert                     | One-shot GPT4o-mini                           |     0.6882      |     0.7632      |     0.7236     \n",
      "distilbert                     | Zero-shot GPT4o                               |     0.6466      |     0.7772      |     0.7058     \n",
      "distilbert                     | CoT GPT4o                                     |     0.6435      |     0.7626      |     0.6978     \n",
      "distilbert                     | Zero-shot GPT4o-mini                          |     0.6304      |     0.7616      |     0.6897     \n",
      "distilbert                     | CoT GPT4o-mini                                |     0.6306      |     0.7527      |     0.6861     \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE_AGG = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\output_bert_score_multi_models_revised.csv' # Output from Script 1\n",
    "OUTPUT_FILE_AGG = r'D:\\liulanqi1\\MindMap-main\\MindMap-main\\bertscore_aggregates_and_reports_revised.csv'\n",
    "\n",
    "# Map original candidate column names (used as identifiers in Script 1's output columns)\n",
    "# to desired display names for the report tables.\n",
    "# Keys MUST match the strings in CANDIDATE_COLUMN_NAMES from Script 1.\n",
    "OUTPUT_NAME_MAP = {\n",
    "    \"Mindmap\": \"Mindmap\",\n",
    "    \"BM25_retrieval\": \"BM25 Retrieval\",\n",
    "    \"KG_retrieval\": \"KG Retrieval\",\n",
    "    \"KG_retrieval_multipath\": \"KG Retrieval Multipath\",\n",
    "    \"KG_self-consistency\": \"KG Self-Consistency\",\n",
    "    \n",
    "    \"Zero_shot_GPT4o_mini_kg\": \"Zero-shot GPT4o-mini (KG)\",\n",
    "    \"zero_shot_gpt4_kg\": \"Zero-shot GPT4o (KG)\",  # 自定义命名\n",
    "    \"Zero_shot_GPT4o_mini\": \"Zero-shot GPT4o-mini\",\n",
    "    \"Zero_shot_GPT4o\": \"Zero-shot GPT4o\",\n",
    "    \n",
    "    \"One_shot_GPT4o_mini\": \"One-shot GPT4o-mini\",\n",
    "    \"One_shot_GPT4o\": \"One-shot GPT4o\",\n",
    "    \n",
    "    \"Few_shot_GPT4o_mini\": \"Few-shot GPT4o-mini\",\n",
    "    \"Few_shot_GPT4o\": \"Few-shot GPT4o\",\n",
    "    \n",
    "    \"CoT_GPT4o_mini\": \"CoT GPT4o-mini\",\n",
    "    \"CoT_GPT4o\": \"CoT GPT4o\",\n",
    "    \n",
    "    \"AutoGen_Single_Result\": \"AutoGen Single\",\n",
    "    \"AutoGen_Single_Result_nokg\": \"AutoGen Single (no KG)\",\n",
    "    \"AutoGen_Multi_Result\": \"AutoGen Multi\",\n",
    "    \n",
    "    \"Multi-Agent Result_nokg\": \"Multi-Agent Result (no KG)\",\n",
    "    \n",
    "    \"KG_ToT_Sampling_Comprehensive_BFS\": \"KG-ToT Samp Comprehensive BFS\",\n",
    "    \"KG_ToT_Sampling_Comprehensive_DFS\": \"KG-ToT Samp Comprehensive DFS\",\n",
    "    \"KG_ToT_Sampling_Comparative_BFS\": \"KG-ToT Samp Comparative BFS\",\n",
    "    \"KG_ToT_Sampling_Comparative_DFS\": \"KG-ToT Samp Comparative DFS\",\n",
    "    \n",
    "    \"KG_ToT_Sequential_Comprehensive_BFS\": \"KG-ToT Seq Comprehensive BFS\",\n",
    "    \"KG_ToT_Sequential_Comprehensive_DFS\": \"KG-ToT Seq Comprehensive DFS\",\n",
    "    \"KG_ToT_Sequential_Comparative_BFS\": \"KG-ToT Seq Comparative BFS\",\n",
    "    \"KG_ToT_Sequential_Comparative_DFS\": \"KG-ToT Seq Comparative DFS\",\n",
    "    \n",
    "    \"Mindmap_nokg\": \"Mindmap (no KG)\"  # 自定义命名\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# These are the identifiers derived from CANDIDATE_COLUMN_NAMES in Script 1\n",
    "# (after basic sanitization if any was applied there, here assumed to be the same)\n",
    "CANDIDATE_IDENTIFIERS = [name.replace(\" \", \"_\").replace(\"-\", \"_\") for name in OUTPUT_NAME_MAP.keys()]\n",
    "\n",
    "\n",
    "# --- Main Script ---\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE_AGG)\n",
    "    print(f\"Successfully read aggregated scores CSV: {INPUT_FILE_AGG}, found {len(df)} rows.\")\n",
    "\n",
    "    # Dynamically detect model prefixes from column names\n",
    "    model_prefixes = set()\n",
    "    for col in df.columns:\n",
    "        # Example column: \"distilbert_Mindmap_precision\"\n",
    "        parts = col.split('_')\n",
    "        # A heuristic: if a part is followed by a known candidate identifier and then a metric\n",
    "        for candidate_id in CANDIDATE_IDENTIFIERS:\n",
    "            if f\"_{candidate_id}_precision\" in col or \\\n",
    "               f\"_{candidate_id}_recall\" in col or \\\n",
    "               f\"_{candidate_id}_f1\" in col:\n",
    "                # Extract the prefix before the first candidate identifier match part\n",
    "                # This is a bit tricky if model prefixes themselves contain underscores.\n",
    "                # Assuming model_short_name from script 1 (e.g. \"distilbert\", \"xlm\", \"roberta\")\n",
    "                # is the prefix we want.\n",
    "                # Let's find the part of the column name before \"_{candidate_id}_METRIC\"\n",
    "                try:\n",
    "                    prefix = col.split(f\"_{candidate_id}_\")[0]\n",
    "                    model_prefixes.add(prefix)\n",
    "                    break # Found prefix for this column\n",
    "                except: # Be robust\n",
    "                    pass\n",
    "    \n",
    "    # A simpler way if prefixes are known (e.g., from MODELS_TO_USE in Script 1)\n",
    "    # model_prefixes = sorted(list(set([m.split(\"-\")[0] for m in [\"distilbert-base-uncased\", \"xlm-roberta-large\", \"roberta-large\"]])))\n",
    "\n",
    "\n",
    "    if not model_prefixes:\n",
    "        # Fallback or error if dynamic detection fails significantly\n",
    "        print(\"Warning: Could not robustly detect model prefixes. Manually define or check column naming.\")\n",
    "        # As a fallback, assuming models from Script 1 if needed:\n",
    "        # models_from_script1 = [\"distilbert-base-uncased\", \"xlm-roberta-large\", \"roberta-large\"]\n",
    "        # model_prefixes = sorted(list(set([m.split(\"-\")[0] for m in models_from_script1])))\n",
    "        # For now, we proceed, but this might indicate an issue if the set is empty.\n",
    "        # Let's try with the initial list of models to get prefixes\n",
    "        _models_for_prefix = [\n",
    "            \"distilbert-base-uncased\", \"xlm-roberta-large\", \"roberta-large\",\n",
    "        ]\n",
    "        model_prefixes = sorted(list(set([m.split(\"-\")[0] for m in _models_for_prefix])))\n",
    "        print(f\"Using predefined model prefixes: {model_prefixes}\")\n",
    "\n",
    "\n",
    "    print(f\"Detected/Using model prefixes: {sorted(list(model_prefixes))}\")\n",
    "\n",
    "    # --- 1. Calculate average score ACROSS BERTScore MODELS for each CANDIDATE METHOD ---\n",
    "    candidate_method_avg_scores = []\n",
    "    for original_candidate_name, display_name in OUTPUT_NAME_MAP.items():\n",
    "        # Use the sanitized/identifier version for column lookup if different\n",
    "        candidate_identifier = original_candidate_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "        metrics_sum = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        valid_models_count = 0\n",
    "        for prefix in model_prefixes:\n",
    "            precision_col = f\"{prefix}_{candidate_identifier}_precision\"\n",
    "            recall_col = f\"{prefix}_{candidate_identifier}_recall\"\n",
    "            f1_col = f\"{prefix}_{candidate_identifier}_f1\"\n",
    "\n",
    "            if all(col_name in df.columns for col_name in [precision_col, recall_col, f1_col]):\n",
    "                # Calculate mean for this specific candidate over all rows in df\n",
    "                avg_p = df[precision_col].mean(skipna=True)\n",
    "                avg_r = df[recall_col].mean(skipna=True)\n",
    "                avg_f1 = df[f1_col].mean(skipna=True)\n",
    "\n",
    "                if not (np.isnan(avg_p) or np.isnan(avg_r) or np.isnan(avg_f1)):\n",
    "                    metrics_sum['precision'] += avg_p\n",
    "                    metrics_sum['recall'] += avg_r\n",
    "                    metrics_sum['f1'] += avg_f1\n",
    "                    valid_models_count += 1\n",
    "            # else:\n",
    "                # print(f\"Debug: Missing columns for {prefix} and {candidate_identifier}\")\n",
    "\n",
    "\n",
    "        if valid_models_count > 0:\n",
    "            candidate_method_avg_scores.append({\n",
    "                'candidate_method_identifier': candidate_identifier, # Store the identifier\n",
    "                'candidate_method_display_name': display_name,\n",
    "                'avg_precision': metrics_sum['precision'] / valid_models_count,\n",
    "                'avg_recall': metrics_sum['recall'] / valid_models_count,\n",
    "                'avg_f1': metrics_sum['f1'] / valid_models_count,\n",
    "                'models_contributed': valid_models_count\n",
    "            })\n",
    "        # else:\n",
    "            # print(f\"Debug: No valid models found for candidate {candidate_identifier}\")\n",
    "\n",
    "\n",
    "    # --- 2. Calculate OVERALL average (average of averages from step 1) ---\n",
    "    overall_avg_metrics = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    if candidate_method_avg_scores:\n",
    "        overall_avg_metrics['precision'] = np.mean([r['avg_precision'] for r in candidate_method_avg_scores if 'avg_precision' in r])\n",
    "        overall_avg_metrics['recall'] = np.mean([r['avg_recall'] for r in candidate_method_avg_scores if 'avg_recall' in r])\n",
    "        overall_avg_metrics['f1'] = np.mean([r['avg_f1'] for r in candidate_method_avg_scores if 'avg_f1' in r])\n",
    "\n",
    "    # --- 3. Per-BERTScore-model averages + detailed per-output per-model ---\n",
    "    per_bertscore_model_avg_scores = [] # For Table 2 (average of a model across all candidate methods)\n",
    "    detailed_model_output_scores = []   # For Table 3\n",
    "\n",
    "    for prefix in sorted(list(model_prefixes)):\n",
    "        # For Table 2: Average this BERTscore model's performance across all candidate methods\n",
    "        model_total_metrics = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        valid_candidates_for_model_count = 0\n",
    "\n",
    "        for original_candidate_name, display_name in OUTPUT_NAME_MAP.items():\n",
    "            candidate_identifier = original_candidate_name.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "            precision_col = f\"{prefix}_{candidate_identifier}_precision\"\n",
    "            recall_col = f\"{prefix}_{candidate_identifier}_recall\"\n",
    "            f1_col = f\"{prefix}_{candidate_identifier}_f1\"\n",
    "\n",
    "            if all(col_name in df.columns for col_name in [precision_col, recall_col, f1_col]):\n",
    "                # Mean score of this model for this candidate_identifier (across all CSV rows)\n",
    "                avg_p = df[precision_col].mean(skipna=True)\n",
    "                avg_r = df[recall_col].mean(skipna=True)\n",
    "                avg_f1 = df[f1_col].mean(skipna=True)\n",
    "\n",
    "                if not (np.isnan(avg_p) or np.isnan(avg_r) or np.isnan(avg_f1)):\n",
    "                    # For Table 3\n",
    "                    detailed_model_output_scores.append({\n",
    "                        'bertscore_model': prefix,\n",
    "                        'candidate_method_identifier': candidate_identifier,\n",
    "                        'candidate_method_display_name': display_name,\n",
    "                        'avg_precision': avg_p,\n",
    "                        'avg_recall': avg_r,\n",
    "                        'avg_f1': avg_f1\n",
    "                    })\n",
    "                    # For Table 2 accumulation\n",
    "                    model_total_metrics['precision'] += avg_p\n",
    "                    model_total_metrics['recall'] += avg_r\n",
    "                    model_total_metrics['f1'] += avg_f1\n",
    "                    valid_candidates_for_model_count += 1\n",
    "\n",
    "        if valid_candidates_for_model_count > 0:\n",
    "            per_bertscore_model_avg_scores.append({\n",
    "                'bertscore_model': prefix,\n",
    "                'avg_precision': model_total_metrics['precision'] / valid_candidates_for_model_count,\n",
    "                'avg_recall': model_total_metrics['recall'] / valid_candidates_for_model_count,\n",
    "                'avg_f1': model_total_metrics['f1'] / valid_candidates_for_model_count,\n",
    "                'candidates_contributed': valid_candidates_for_model_count\n",
    "            })\n",
    "\n",
    "    # --- Save combined results to a CSV (optional, for easier data handling) ---\n",
    "    # Create DataFrames from the lists of dictionaries\n",
    "    df_candidate_avg = pd.DataFrame(candidate_method_avg_scores)\n",
    "    df_bertscore_model_avg = pd.DataFrame(per_bertscore_model_avg_scores)\n",
    "    df_detailed = pd.DataFrame(detailed_model_output_scores)\n",
    "\n",
    "    # You might want to save these separately or combine them meaningfully\n",
    "    # For now, let's just print them. If saving, decide on format.\n",
    "    # Example of saving one:\n",
    "    if not df_candidate_avg.empty:\n",
    "      df_candidate_avg.to_csv(OUTPUT_FILE_AGG, index=False, float_format='%.6f')\n",
    "      print(f\"\\nCandidate method average scores saved to {OUTPUT_FILE_AGG}\")\n",
    "\n",
    "\n",
    "    # --- Printing Tables ---\n",
    "\n",
    "    # Table 1: Average BERTScore for each CANDIDATE METHOD (across BERTscore models), sorted by F1\n",
    "    print(\"\\n--- Table 1: Avg BERTScore per Candidate Method (across BERTScore Models) ---\")\n",
    "    print(\"Sorted by F1 score (descending)\")\n",
    "    print(\"=\" * 100)\n",
    "    header = f\"{'Candidate Method':<45} | {'Avg Precision':^15} | {'Avg Recall':^15} | {'Avg F1':^15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    if candidate_method_avg_scores:\n",
    "        sorted_candidates = sorted(candidate_method_avg_scores, key=lambda x: x.get('avg_f1', 0), reverse=True)\n",
    "        for r_cand in sorted_candidates:\n",
    "            print(f\"{r_cand.get('candidate_method_display_name', 'N/A'):<45} | {r_cand.get('avg_precision', 0):^15.4f} | {r_cand.get('avg_recall', 0):^15.4f} | {r_cand.get('avg_f1', 0):^15.4f}\")\n",
    "        print(\"-\" * len(header))\n",
    "        # Print overall average of these candidate averages\n",
    "        if overall_avg_metrics['f1'] != 0.0 : # Check if overall_avg was computed\n",
    "             print(f\"{'OVERALL (Avg of above)':<45} | {overall_avg_metrics['precision']:^15.4f} | {overall_avg_metrics['recall']:^15.4f} | {overall_avg_metrics['f1']:^15.4f}\")\n",
    "        print(\"-\" * len(header))\n",
    "    else:\n",
    "        print(\"No data for candidate method averages.\")\n",
    "\n",
    "\n",
    "    # Table 2: Average BERTScore for each BERTScore MODEL (across candidate methods), sorted by F1\n",
    "    print(\"\\n--- Table 2: Avg BERTScore per BERTScore Model (across Candidate Methods) ---\")\n",
    "    print(\"Sorted by F1 score (descending)\")\n",
    "    print(\"=\" * 90)\n",
    "    header_m = f\"{'BERTScore Model':<30} | {'Avg Precision':^15} | {'Avg Recall':^15} | {'Avg F1':^15}\"\n",
    "    print(header_m)\n",
    "    print(\"-\" * len(header_m))\n",
    "    if per_bertscore_model_avg_scores:\n",
    "        sorted_models = sorted(per_bertscore_model_avg_scores, key=lambda x: x.get('avg_f1', 0), reverse=True)\n",
    "        for r_model in sorted_models:\n",
    "            print(f\"{r_model.get('bertscore_model', 'N/A'):<30} | {r_model.get('avg_precision', 0):^15.4f} | {r_model.get('avg_recall', 0):^15.4f} | {r_model.get('avg_f1', 0):^15.4f}\")\n",
    "        print(\"-\" * len(header_m))\n",
    "    else:\n",
    "        print(\"No data for BERTScore model averages.\")\n",
    "\n",
    "\n",
    "    # Table 3: Detailed BERTScore for each BERTScore MODEL on each CANDIDATE METHOD, sorted by F1\n",
    "    print(\"\\n--- Table 3: Detailed Avg BERTScore (BERTScore Model vs. Candidate Method) ---\")\n",
    "    print(\"Sorted by F1 score (descending)\")\n",
    "    print(\"=\" * 120)\n",
    "    header_d = f\"{'BERTScore Model':<30} | {'Candidate Method':<45} | {'Avg Precision':^15} | {'Avg Recall':^15} | {'Avg F1':^15}\"\n",
    "    print(header_d)\n",
    "    print(\"-\" * len(header_d))\n",
    "    if detailed_model_output_scores:\n",
    "        # Use the DataFrame for easier sorting if created: df_detailed\n",
    "        if not df_detailed.empty:\n",
    "            sorted_detailed = df_detailed.sort_values(by='avg_f1', ascending=False).fillna(0)\n",
    "            for _, d_row in sorted_detailed.iterrows():\n",
    "                print(f\"{d_row.get('bertscore_model', 'N/A'):<30} | {d_row.get('candidate_method_display_name', 'N/A'):<45} | {d_row.get('avg_precision', 0):^15.4f} | {d_row.get('avg_recall', 0):^15.4f} | {d_row.get('avg_f1', 0):^15.4f}\")\n",
    "        print(\"-\" * len(header_d))\n",
    "    else:\n",
    "        print(\"No data for detailed model vs. candidate method scores.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{INPUT_FILE_AGG}' not found.\")\n",
    "except Exception as e_agg:\n",
    "    print(f\"An error occurred during aggregation: {e_agg}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
