{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46f869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Script 1: Generating Rankings...\n",
      "Identified 12 method columns for ranking: ['llama_pred_nolora', 'llama_pred', 'deepseek_pred_nolora', 'deepseek_pred', 'Zero_shot_GPT4o_mini', 'Zero_shot_GPT4o', 'One_shot_GPT4o_mini', 'One_shot_GPT4o', 'Few_shot_GPT4o_mini', 'Few_shot_GPT4o', 'CoT_GPT4o_mini', 'CoT_GPT4o']\n",
      "Saved method column order to: D:\\liulanqi1\\MindMap-main\\MindMap-main\\ranked_method_column_order.json\n",
      "\n",
      "--- Processing with API model: gpt-4o (file tag: gpt_4o) ---\n",
      "Processing Row 1 for model gpt-4o with 12 outputs...\n",
      "Processing Row 2 for model gpt-4o with 12 outputs...\n",
      "Processing Row 3 for model gpt-4o with 12 outputs...\n",
      "Processing Row 4 for model gpt-4o with 12 outputs...\n",
      "Processing Row 5 for model gpt-4o with 12 outputs...\n",
      "Processing Row 6 for model gpt-4o with 12 outputs...\n",
      "Processing Row 7 for model gpt-4o with 12 outputs...\n",
      "Processing Row 8 for model gpt-4o with 12 outputs...\n",
      "Processing Row 9 for model gpt-4o with 12 outputs...\n",
      "Processing Row 10 for model gpt-4o with 12 outputs...\n",
      "Processing Row 11 for model gpt-4o with 12 outputs...\n",
      "Processing Row 12 for model gpt-4o with 12 outputs...\n",
      "Processing Row 13 for model gpt-4o with 12 outputs...\n",
      "Processing Row 14 for model gpt-4o with 12 outputs...\n",
      "Processing Row 15 for model gpt-4o with 12 outputs...\n",
      "Processing Row 16 for model gpt-4o with 12 outputs...\n",
      "Processing Row 17 for model gpt-4o with 12 outputs...\n",
      "Processing Row 18 for model gpt-4o with 12 outputs...\n",
      "Processing Row 19 for model gpt-4o with 12 outputs...\n",
      "Processing Row 20 for model gpt-4o with 12 outputs...\n",
      "Processing Row 21 for model gpt-4o with 12 outputs...\n",
      "Processing Row 22 for model gpt-4o with 12 outputs...\n",
      "Processing Row 23 for model gpt-4o with 12 outputs...\n",
      "Processing Row 24 for model gpt-4o with 12 outputs...\n",
      "Processing Row 25 for model gpt-4o with 12 outputs...\n",
      "Processing Row 26 for model gpt-4o with 12 outputs...\n",
      "Processing Row 27 for model gpt-4o with 12 outputs...\n",
      "Processing Row 28 for model gpt-4o with 12 outputs...\n",
      "Processing Row 29 for model gpt-4o with 12 outputs...\n",
      "Processing Row 30 for model gpt-4o with 12 outputs...\n",
      "Processing Row 31 for model gpt-4o with 12 outputs...\n",
      "Processing Row 32 for model gpt-4o with 12 outputs...\n",
      "Processing Row 33 for model gpt-4o with 12 outputs...\n",
      "Processing Row 34 for model gpt-4o with 12 outputs...\n",
      "Processing Row 35 for model gpt-4o with 12 outputs...\n",
      "Processing Row 36 for model gpt-4o with 12 outputs...\n",
      "Processing Row 37 for model gpt-4o with 12 outputs...\n",
      "Processing Row 38 for model gpt-4o with 12 outputs...\n",
      "Processing Row 39 for model gpt-4o with 12 outputs...\n",
      "Processing Row 40 for model gpt-4o with 12 outputs...\n",
      "Processing Row 41 for model gpt-4o with 12 outputs...\n",
      "Processing Row 42 for model gpt-4o with 12 outputs...\n",
      "Processing Row 43 for model gpt-4o with 12 outputs...\n",
      "Processing Row 44 for model gpt-4o with 12 outputs...\n",
      "Processing Row 45 for model gpt-4o with 12 outputs...\n",
      "Processing Row 46 for model gpt-4o with 12 outputs...\n",
      "Processing Row 47 for model gpt-4o with 12 outputs...\n",
      "Processing Row 48 for model gpt-4o with 12 outputs...\n",
      "Processing Row 49 for model gpt-4o with 12 outputs...\n",
      "Processing Row 50 for model gpt-4o with 12 outputs...\n",
      "Processing Row 51 for model gpt-4o with 12 outputs...\n",
      "Processing Row 52 for model gpt-4o with 12 outputs...\n",
      "Processing Row 53 for model gpt-4o with 12 outputs...\n",
      "Processing Row 54 for model gpt-4o with 12 outputs...\n",
      "Processing Row 55 for model gpt-4o with 12 outputs...\n",
      "Processing Row 56 for model gpt-4o with 12 outputs...\n",
      "Processing Row 57 for model gpt-4o with 12 outputs...\n",
      "Processing Row 58 for model gpt-4o with 12 outputs...\n",
      "Processing Row 59 for model gpt-4o with 12 outputs...\n",
      "Processing Row 60 for model gpt-4o with 12 outputs...\n",
      "\n",
      "--- Processing with API model: gpt-4o-mini (file tag: gpt_4o_mini) ---\n",
      "Processing Row 1 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 2 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 3 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 4 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 5 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 6 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 7 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 8 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 9 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 10 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 11 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 12 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 13 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 14 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 15 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 16 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 17 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 18 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 19 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 20 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 21 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 22 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 23 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 24 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 25 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 26 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 27 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 28 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 29 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 30 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 31 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 32 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 33 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 34 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 35 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 36 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 37 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 38 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 39 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 40 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 41 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 42 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 43 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 44 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 45 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 46 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 47 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 48 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 49 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 50 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 51 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 52 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 53 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 54 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 55 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 56 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 57 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 58 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 59 for model gpt-4o-mini with 12 outputs...\n",
      "Processing Row 60 for model gpt-4o-mini with 12 outputs...\n",
      "\n",
      "--- Processing with API model: gpt-4.1 (file tag: gpt_4dot1) ---\n",
      "Processing Row 1 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 2 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 3 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 4 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 5 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 6 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 7 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 8 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 9 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 10 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 11 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 12 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 13 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 14 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 15 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 16 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 17 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 18 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 19 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 20 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 21 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 22 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 23 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 24 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 25 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 26 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 27 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 28 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 29 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 30 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 31 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 32 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 33 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 34 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 35 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 36 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 37 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 38 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 39 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 40 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 41 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 42 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 43 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 44 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 45 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 46 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 47 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 48 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 49 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 50 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 51 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 52 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 53 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 54 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 55 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 56 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 57 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 58 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 59 for model gpt-4.1 with 12 outputs...\n",
      "Processing Row 60 for model gpt-4.1 with 12 outputs...\n",
      "\n",
      "Script 1: Ranking generation complete.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import csv\n",
    "from time import sleep\n",
    "import json\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "openai.api_key = \"" # Your API Key\n",
    "\n",
    "input_csv_file = r'D:\\浏览器\\merged_predictions_all_cols.csv'\n",
    "output_dir = r'D:\\liulanqi1\\MindMap-main\\MindMap-main'\n",
    "method_mapping_file = fr'{output_dir}\\ranked_method_column_order.json'\n",
    "\n",
    "# Use original model names for clarity and API calls\n",
    "ORIGINAL_EVALUATION_MODELS = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-4.1\"]\n",
    "\n",
    "def get_model_filename_tag(original_model_name):\n",
    "    \"\"\"Creates a safe filename tag from the original model name.\"\"\"\n",
    "    return original_model_name.replace(\"-\", \"_\").replace(\".\", \"dot\")\n",
    "\n",
    "# --- Helper Functions (get_method_columns, generate_dynamic_prompt, prompt_ranking_dynamic) ---\n",
    "# (Keep these functions as they were in the previous good version)\n",
    "# def get_method_columns(header_row):\n",
    "#     excluded_suffixes = (\"_paths\", \"_time\",\"_confidence\")\n",
    "#     excluded_exact = (\"Question\", \"Label\")\n",
    "#     method_cols = [col for col in header_row if col not in excluded_exact and not col.endswith(excluded_suffixes)]\n",
    "#     print(f\"Identified {len(method_cols)} method columns for ranking: {method_cols}\")\n",
    "#     return method_cols\n",
    "\n",
    "def get_method_columns(header_row):\n",
    "    excluded_suffixes = (\"_paths\", \"_time\", \"_confidence\")\n",
    "    excluded_exact = (\"question\", \"label\")  # 可根据需要扩展\n",
    "    method_cols = [\n",
    "        col for col in header_row[1:]  # 从第二列开始\n",
    "        if col not in excluded_exact and not col.endswith(excluded_suffixes) and \"Status\" not in col\n",
    "    ]\n",
    "    print(f\"Identified {len(method_cols)} method columns for ranking: {method_cols}\")\n",
    "    return method_cols\n",
    "\n",
    "\n",
    "# def generate_dynamic_prompt(reference_text, output_texts_list):\n",
    "#     num_outputs = len(output_texts_list)\n",
    "#     outputs_section_parts = []\n",
    "#     format_args = {'reference': reference_text}\n",
    "#     for i, text in enumerate(output_texts_list):\n",
    "#         key = f\"output{i+1}\"\n",
    "#         outputs_section_parts.append(f\"{key}: {{output_val{i}}}\")\n",
    "#         format_args[f'output_val{i}'] = text\n",
    "#     outputs_section = \"\\n\\n\".join(outputs_section_parts)\n",
    "#     example_format_lines = [f\"{j}. Output{j}\" for j in range(1, num_outputs + 1)]\n",
    "#     example_numbers_list = list(range(1, num_outputs + 1))\n",
    "#     random.shuffle(example_numbers_list)\n",
    "#     example_ranking_numbers = \",\".join(map(str, example_numbers_list))\n",
    "#     prompt_template = f\"\"\"\n",
    "# Reference: {{reference}}\n",
    "\n",
    "# {outputs_section}\n",
    "\n",
    "# According to the facts of the disease and the drug and test recommendations in reference output, order the fact match of the output from highest to lowest.\n",
    "\n",
    "# The output followed the format bellow (this is just an example of structure, not content):\n",
    "# \"\"\" + \"\\n\".join(example_format_lines) + f\"\"\"\n",
    "\n",
    "# According to the facts of the disease and the drug and test recommendations in reference output, rank the outputs from highest to lowest fact match.\n",
    "\n",
    "# ⚠️ IMPORTANT: Only output the final ranking as numbers separated by commas, no explanation, no extra text. The numbers should correspond to the output numbers (e.g., if Output3 is best, Output1 is second, etc., for {num_outputs} outputs, you might output something like '3,1,...').\n",
    "# Example for {num_outputs} outputs: {example_ranking_numbers}\n",
    "# \"\"\"\n",
    "#     return prompt_template.format(**format_args)\n",
    "import random # Ensure random is imported\n",
    "\n",
    "def generate_dynamic_prompt(reference_text, output_texts_list):\n",
    "    num_outputs = len(output_texts_list)\n",
    "    outputs_section_parts = []\n",
    "    # Method names are implicitly Output1, Output2, etc., based on their order in output_texts_list\n",
    "    \n",
    "    # Construct the numbered list of outputs for the LLM to refer to\n",
    "    for i, text in enumerate(output_texts_list):\n",
    "        outputs_section_parts.append(f\"Output {i+1}:\\n\\\"\\\"\\\"\\n{text}\\n\\\"\\\"\\\"\")\n",
    "    \n",
    "    outputs_section = \"\\n\\n\".join(outputs_section_parts)\n",
    "    \n",
    "    # Create a randomized example ranking format for illustration\n",
    "    example_numbers_list = list(range(1, num_outputs + 1))\n",
    "    random.shuffle(example_numbers_list)\n",
    "    example_ranking_numbers = \",\".join(map(str, example_numbers_list))\n",
    "\n",
    "    prompt = f\"\"\"You are an expert medical evaluator. Your task is to rank the following {num_outputs} machine-generated medical answers based on their quality compared to a reference answer.\n",
    "\n",
    "Reference Answer:\n",
    "\\\"\\\"\\\"\n",
    "{reference_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Machine-Generated Outputs to Rank:\n",
    "{outputs_section}\n",
    "\n",
    "Evaluation Criteria for Ranking (consider these holistically for each output):\n",
    "\n",
    "1.  **Diagnostic Accuracy & Safety**:\n",
    "    * How accurate is the primary diagnosis compared to the reference?\n",
    "    * Are the differential diagnoses relevant and correctly prioritized?\n",
    "    * Are the recommended tests appropriate and safe for the given symptoms and potential diagnoses?\n",
    "    * Are treatment/medication suggestions safe, evidence-based, and appropriate for the diagnoses discussed?\n",
    "    * Does the output avoid harmful or misleading medical advice?\n",
    "\n",
    "2.  **Completeness & Relevance**:\n",
    "    * Does the output address all key aspects of the patient's query implicitly or explicitly covered in the reference (e.g., diagnosis, tests, treatment)?\n",
    "    * Is all information provided clinically relevant to the patient's presentation and the reference?\n",
    "    * Does it include critical information present in the reference?\n",
    "\n",
    "3.  **Reasoning & Justification**:\n",
    "    * Is the medical reasoning sound, logical, and easy to follow?\n",
    "    * Are diagnoses, tests, and treatments adequately justified?\n",
    "    * Does the output explain *why* certain recommendations are made, similar to or better than the reference?\n",
    "\n",
    "4.  **Clarity & Fluency**:\n",
    "    * Is the language clear, concise, and unambiguous for a patient or general practitioner?\n",
    "    * Is the information well-organized and easy to understand?\n",
    "\n",
    "5.  **Novelty & Added Value (Bonus)**:\n",
    "    * Does the output provide any *additional correct, relevant, and safe* medical insights, tests, or treatment considerations not mentioned in the reference that could be genuinely helpful? (This is a bonus; factual alignment with the reference on core points is primary).\n",
    "\n",
    "Task:\n",
    "Based on a holistic assessment of the above criteria, rank the outputs from best to worst. The best output is Rank 1.\n",
    "\n",
    "⚠️ **CRITICAL OUTPUT FORMAT**: Provide ONLY the final ranking as a comma-separated list of numbers (e.g., \"3,1,2,{example_ranking_numbers[6:]}\" if Output 3 is best, Output 1 is second, etc.). The numbers correspond to the 'Output X' labels. Do NOT include any other text, explanations, or headers.\n",
    "\n",
    "Example for {num_outputs} outputs (random order): {example_ranking_numbers}\n",
    "\n",
    "Your Ranking:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def prompt_ranking_dynamic(reference_text, output_texts_list, model_name=\"gpt-4o\"):\n",
    "    prompt = generate_dynamic_prompt(reference_text, output_texts_list)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an excellent evaluator. Follow instructions precisely.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "    top_p=1,\n",
    "n=1,\n",
    "presence_penalty=0,\n",
    "frequency_penalty=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# --- Main Script Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Script 1: Generating Rankings...\")\n",
    "    with open(input_csv_file, 'r', newline=\"\", encoding='utf-8') as f_input_header:\n",
    "        reader_header = csv.reader(f_input_header)\n",
    "        header = next(reader_header)\n",
    "        actual_method_columns = get_method_columns(header)\n",
    "        if not actual_method_columns:\n",
    "            print(\"Error: No method columns identified.\")\n",
    "            exit()\n",
    "        with open(method_mapping_file, 'w', encoding='utf-8') as f_map:\n",
    "            json.dump(actual_method_columns, f_map)\n",
    "        print(f\"Saved method column order to: {method_mapping_file}\")\n",
    "\n",
    "    for original_model_name in ORIGINAL_EVALUATION_MODELS:\n",
    "        model_api_name = original_model_name # Name to use for OpenAI API call\n",
    "        model_file_tag = get_model_filename_tag(original_model_name) # Tag for filename\n",
    "\n",
    "        print(f\"\\n--- Processing with API model: {model_api_name} (file tag: {model_file_tag}) ---\")\n",
    "        rankings_output_file = fr'{output_dir}\\rankings_only_{model_file_tag}.csv'\n",
    "\n",
    "        with open(input_csv_file, 'r', newline=\"\", encoding='utf-8') as f_input, \\\n",
    "             open(rankings_output_file, 'w', newline='', encoding='utf-8') as f_rankings:\n",
    "            reader = csv.reader(f_input)\n",
    "            ranking_writer = csv.writer(f_rankings)\n",
    "            current_header = next(reader)\n",
    "            try:\n",
    "                reference_col_idx = current_header.index(\"label\")\n",
    "            except ValueError:\n",
    "                print(f\"Error: 'Label' column not found for model {model_api_name}. Skipping.\")\n",
    "                continue\n",
    "            method_col_indices = [current_header.index(col) for col in actual_method_columns]\n",
    "            \n",
    "            # Use the original model name for the CSV column header for clarity\n",
    "            ranking_writer.writerow([\"RowNumber\", f\"{model_api_name.upper()}_Ranking\"])\n",
    "\n",
    "            row_number = 1\n",
    "            for row_idx, row in enumerate(reader):\n",
    "                try:\n",
    "                    reference_text = row[reference_col_idx].strip(\"\\n\")\n",
    "                    current_output_texts = [row[col_idx].strip(\"\\n\") for col_idx in method_col_indices]\n",
    "                    if not current_output_texts:\n",
    "                        ranking_writer.writerow([row_number, \"NO_OUTPUTS_FOUND\"])\n",
    "                        row_number += 1\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Processing Row {row_number} for model {model_api_name} with {len(current_output_texts)} outputs...\")\n",
    "                    ranking_str = \"\"\n",
    "                    # ... (rest of the try/except/retry logic for prompt_ranking_dynamic)\n",
    "                    # Ensure prompt_ranking_dynamic is called with model_api_name\n",
    "                    success = False\n",
    "                    attempts = 0\n",
    "                    max_attempts = 3\n",
    "                    while not success and attempts < max_attempts:\n",
    "                        try:\n",
    "                            ranking_str = prompt_ranking_dynamic(\n",
    "                                reference_text, \n",
    "                                current_output_texts,\n",
    "                                model_name=model_api_name # Use model_api_name here\n",
    "                            )\n",
    "                            parts = ranking_str.split(',')\n",
    "                            if len(parts) == len(current_output_texts) and all(p.strip().isdigit() for p in parts):\n",
    "                                success = True\n",
    "                            else:\n",
    "                                ranking_str = f\"INVALID_FORMAT_{ranking_str}\"\n",
    "                                success = True \n",
    "                        except Exception as e:\n",
    "                            attempts += 1\n",
    "                            print(f\"Error for Model {model_api_name}, Row {row_number}: {e}. Attempt {attempts}/{max_attempts}.\")\n",
    "                            if attempts < max_attempts: sleep(40)\n",
    "                            else: ranking_str = \"FAILED_ATTEMPTS\"; success = True\n",
    "                    # ...\n",
    "                    ranking_writer.writerow([row_number, ranking_str])\n",
    "                    row_number += 1\n",
    "                    sleep(1) # API rate limit politeness\n",
    "                except IndexError:\n",
    "                    ranking_writer.writerow([row_number if 'row_number' in locals() else row_idx +1, \"ROW_INDEX_ERROR\"])\n",
    "                    if 'row_number' in locals(): row_number +=1\n",
    "                except Exception as e_outer:\n",
    "                    ranking_writer.writerow([row_number if 'row_number' in locals() else row_idx +1, \"OUTER_PROCESSING_ERROR\"])\n",
    "                    if 'row_number' in locals(): row_number +=1\n",
    "    print(\"\\nScript 1: Ranking generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122f281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a938944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Script 2: Processing Ranking Files...\n",
      "\n",
      "--- Processing file for original model: gpt-4o (using file tag: gpt_4o) ---\n",
      "Input file: D:\\liulanqi1\\MindMap-main\\MindMap-main\\rankings_only_gpt_4o.csv\n",
      "Determined/Inferred 12 outputs for model gpt-4o.\n",
      "\n",
      "--- Processing file for original model: gpt-4o-mini (using file tag: gpt_4o_mini) ---\n",
      "Input file: D:\\liulanqi1\\MindMap-main\\MindMap-main\\rankings_only_gpt_4o_mini.csv\n",
      "Determined/Inferred 12 outputs for model gpt-4o-mini.\n",
      "\n",
      "--- Processing file for original model: gpt-4.1 (using file tag: gpt_4dot1) ---\n",
      "Input file: D:\\liulanqi1\\MindMap-main\\MindMap-main\\rankings_only_gpt_4dot1.csv\n",
      "Determined/Inferred 12 outputs for model gpt-4.1.\n",
      "\n",
      "Script 2: Processing of ranking files complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# --- Configuration ---\n",
    "input_output_dir = r'D:\\liulanqi1\\MindMap-main\\MindMap-main'\n",
    "\n",
    "# Use original model names here as well, to derive the tags consistently\n",
    "ORIGINAL_EVALUATION_MODELS = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-4.1\"] # Same as Script 1\n",
    "\n",
    "def get_model_filename_tag(original_model_name):\n",
    "    \"\"\"Creates a safe filename tag from the original model name.\"\"\"\n",
    "    # ENSURE THIS FUNCTION IS IDENTICAL TO THE ONE IN SCRIPT 1\n",
    "    return original_model_name.replace(\"-\", \"_\").replace(\".\", \"dot\")\n",
    "\n",
    "# --- Main Script Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Script 2: Processing Ranking Files...\")\n",
    "\n",
    "    for original_model_name in ORIGINAL_EVALUATION_MODELS:\n",
    "        model_file_tag = get_model_filename_tag(original_model_name) # Generate the tag\n",
    "\n",
    "        input_ranking_file = fr'{input_output_dir}\\rankings_only_{model_file_tag}.csv'\n",
    "        output_computed_file = fr'{input_output_dir}\\output_ranking_comput_{model_file_tag}.csv'\n",
    "        \n",
    "        print(f\"\\n--- Processing file for original model: {original_model_name} (using file tag: {model_file_tag}) ---\")\n",
    "        print(f\"Input file: {input_ranking_file}\")\n",
    "        # ... (rest of Script 2 logic remains largely the same) ...\n",
    "        # No changes needed inside the loop for reading/writing rows,\n",
    "        # as it dynamically determines num_outputs and processes the ranking string.\n",
    "        try:\n",
    "            with open(input_ranking_file, 'r', newline=\"\", encoding='utf-8') as f_input, \\\n",
    "                 open(output_computed_file, 'w', newline='', encoding='utf-8') as f_output:\n",
    "                reader = csv.reader(f_input)\n",
    "                writer = csv.writer(f_output)\n",
    "                header = next(reader)\n",
    "                num_outputs = 0\n",
    "                # Temporarily reopen to determine num_outputs safely\n",
    "                with open(input_ranking_file, 'r', newline=\"\", encoding='utf-8') as f_temp_input:\n",
    "                    temp_reader_for_num_outputs = csv.reader(f_temp_input)\n",
    "                    next(temp_reader_for_num_outputs) \n",
    "                    for temp_row in temp_reader_for_num_outputs:\n",
    "                        if len(temp_row) > 1:\n",
    "                            ranking_str_sample = temp_row[1]\n",
    "                            if not any(err_tag in ranking_str_sample for err_tag in [\"FAILED\", \"ERROR\", \"INVALID\", \"NO_OUTPUTS\"]):\n",
    "                                numbers = [num.strip() for num in ranking_str_sample.split(',') if num.strip().isdigit()]\n",
    "                                if numbers: num_outputs = len(numbers); break\n",
    "                    if num_outputs == 0: # Fallback\n",
    "                         with open(input_ranking_file, 'r', newline=\"\", encoding='utf-8') as f_temp_fallback:\n",
    "                            temp_reader_fallback = csv.reader(f_temp_fallback)\n",
    "                            next(temp_reader_fallback)\n",
    "                            for temp_row_fallback in temp_reader_fallback:\n",
    "                                if len(temp_row_fallback) > 1:\n",
    "                                    parts = temp_row_fallback[1].split(',')\n",
    "                                    if len(parts) > 1: num_outputs = len(parts); break\n",
    "                    if num_outputs == 0:\n",
    "                        print(f\"Error: Could not determine num_outputs for {original_model_name}. Skipping.\")\n",
    "                        continue\n",
    "                print(f\"Determined/Inferred {num_outputs} outputs for model {original_model_name}.\")\n",
    "                extended_header = list(header)\n",
    "                extended_header.extend([f\"output{i+1}_ranking\" for i in range(num_outputs)])\n",
    "                writer.writerow(extended_header)\n",
    "                # ... (The rest of the row processing loop from your previous good version of Script 2)\n",
    "                for row_idx, row in enumerate(reader): # reader is already past header\n",
    "                    try:\n",
    "                        # ... (inner try-except for row processing, same as before)\n",
    "                        row_number_text = row[0]\n",
    "                        ranking_str = row[1]\n",
    "                        rankings_to_add = [0] * num_outputs \n",
    "                        if any(err_tag in ranking_str for err_tag in [\"FAILED\", \"ERROR\", \"INVALID\", \"NO_OUTPUTS\"]):\n",
    "                            pass # rankings_to_add remains zeros\n",
    "                        else:\n",
    "                            try:\n",
    "                                numbers = [int(num.strip()) for num in ranking_str.split(',') if num.strip().isdigit()]\n",
    "                                if len(numbers) == num_outputs:\n",
    "                                    output_to_actual_rank = {output_idx: rank_pos for rank_pos, output_idx in enumerate(numbers, start=1)}\n",
    "                                    for i in range(1, num_outputs + 1):\n",
    "                                        rankings_to_add[i-1] = output_to_actual_rank.get(i, 0)\n",
    "                            except ValueError: pass # rankings_to_add remains zeros\n",
    "                        new_row = list(row)\n",
    "                        new_row.extend(rankings_to_add)\n",
    "                        writer.writerow(new_row)\n",
    "                    except Exception as e_inner:\n",
    "                        writer.writerow(row + [0] * num_outputs)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Input file not found: {input_ranking_file}. Skipping model {original_model_name}.\")\n",
    "        except Exception as e_outer:\n",
    "            print(f\"An unexpected error occurred for {original_model_name}: {e_outer}\")\n",
    "    print(\"\\nScript 2: Processing of ranking files complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c136136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Script 3: Calculating Average Ranks and Presenting Results...\n",
      "\n",
      "\n",
      "=== Results for Evaluation Model: GPT-4O ===\n",
      "\n",
      "--- Average Rankings (lower is better) ---\n",
      "1. Zero_shot_GPT4o_mini: 3.05 (from 59/60)\n",
      "2. Zero_shot_GPT4o: 4.02 (from 59/60)\n",
      "3. llama_pred: 5.25 (from 59/60)\n",
      "4. One_shot_GPT4o_mini: 5.80 (from 59/60)\n",
      "5. deepseek_pred: 6.66 (from 59/60)\n",
      "6. One_shot_GPT4o: 6.80 (from 59/60)\n",
      "7. deepseek_pred_nolora: 6.83 (from 59/60)\n",
      "8. llama_pred_nolora: 6.98 (from 59/60)\n",
      "9. Few_shot_GPT4o_mini: 7.66 (from 59/60)\n",
      "10. CoT_GPT4o_mini: 7.73 (from 59/60)\n",
      "11. CoT_GPT4o: 8.59 (from 59/60)\n",
      "12. Few_shot_GPT4o: 8.63 (from 59/60)\n",
      "\n",
      "\n",
      "=== Results for Evaluation Model: GPT-4O-MINI ===\n",
      "\n",
      "--- Average Rankings (lower is better) ---\n",
      "1. Zero_shot_GPT4o_mini: 3.07 (from 60/60)\n",
      "2. Zero_shot_GPT4o: 4.07 (from 60/60)\n",
      "3. One_shot_GPT4o_mini: 5.43 (from 60/60)\n",
      "4. deepseek_pred: 5.82 (from 60/60)\n",
      "5. llama_pred: 6.05 (from 60/60)\n",
      "6. One_shot_GPT4o: 6.72 (from 60/60)\n",
      "7. llama_pred_nolora: 6.78 (from 60/60)\n",
      "8. deepseek_pred_nolora: 6.82 (from 60/60)\n",
      "9. Few_shot_GPT4o_mini: 7.25 (from 60/60)\n",
      "10. Few_shot_GPT4o: 8.00 (from 60/60)\n",
      "11. CoT_GPT4o_mini: 8.52 (from 60/60)\n",
      "12. CoT_GPT4o: 9.48 (from 60/60)\n",
      "\n",
      "\n",
      "=== Results for Evaluation Model: GPT-4.1 ===\n",
      "\n",
      "--- Average Rankings (lower is better) ---\n",
      "1. Zero_shot_GPT4o_mini: 3.55 (from 60/60)\n",
      "2. Zero_shot_GPT4o: 4.95 (from 60/60)\n",
      "3. One_shot_GPT4o_mini: 5.88 (from 60/60)\n",
      "4. deepseek_pred: 5.92 (from 60/60)\n",
      "5. deepseek_pred_nolora: 6.17 (from 60/60)\n",
      "6. llama_pred: 6.30 (from 60/60)\n",
      "7. CoT_GPT4o_mini: 6.48 (from 60/60)\n",
      "8. llama_pred_nolora: 7.08 (from 60/60)\n",
      "9. One_shot_GPT4o: 7.32 (from 60/60)\n",
      "10. Few_shot_GPT4o_mini: 7.72 (from 60/60)\n",
      "11. CoT_GPT4o: 7.75 (from 60/60)\n",
      "12. Few_shot_GPT4o: 8.88 (from 60/60)\n",
      "\n",
      "\n",
      "--- Consolidated Summary Across All Evaluation Models ---\n",
      "Method                                  GPT-4O              GPT-4O-MINI         GPT-4.1             \n",
      "----------------------------------------------------------------------------------------------------\n",
      "llama_pred_nolora                       6.98 (59)           6.78 (60)           7.08 (60)           \n",
      "llama_pred                              5.25 (59)           6.05 (60)           6.30 (60)           \n",
      "deepseek_pred_nolora                    6.83 (59)           6.82 (60)           6.17 (60)           \n",
      "deepseek_pred                           6.66 (59)           5.82 (60)           5.92 (60)           \n",
      "Zero_shot_GPT4o_mini                    3.05 (59)           3.07 (60)           3.55 (60)           \n",
      "Zero_shot_GPT4o                         4.02 (59)           4.07 (60)           4.95 (60)           \n",
      "One_shot_GPT4o_mini                     5.80 (59)           5.43 (60)           5.88 (60)           \n",
      "One_shot_GPT4o                          6.80 (59)           6.72 (60)           7.32 (60)           \n",
      "Few_shot_GPT4o_mini                     7.66 (59)           7.25 (60)           7.72 (60)           \n",
      "Few_shot_GPT4o                          8.63 (59)           8.00 (60)           8.88 (60)           \n",
      "CoT_GPT4o_mini                          7.73 (59)           8.52 (60)           6.48 (60)           \n",
      "CoT_GPT4o                               8.59 (59)           9.48 (60)           7.75 (60)           \n",
      "\n",
      "Script 3: Reporting complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "results_dir = r'D:\\liulanqi1\\MindMap-main\\MindMap-main'\n",
    "method_mapping_file_path = fr'{results_dir}\\ranked_method_column_order.json'\n",
    "\n",
    "# Use original model names here as well\n",
    "ORIGINAL_EVALUATION_MODELS = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-4.1\"] # Same as Script 1 & 2\n",
    "\n",
    "def get_model_filename_tag(original_model_name):\n",
    "    \"\"\"Creates a safe filename tag from the original model name.\"\"\"\n",
    "    # ENSURE THIS FUNCTION IS IDENTICAL TO THE ONE IN SCRIPT 1 & 2\n",
    "    return original_model_name.replace(\"-\", \"_\").replace(\".\", \"dot\")\n",
    "\n",
    "# --- Main Script Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Script 3: Calculating Average Ranks and Presenting Results...\")\n",
    "    try:\n",
    "        with open(method_mapping_file_path, 'r', encoding='utf-8') as f_map:\n",
    "            actual_method_columns_ordered = json.load(f_map)\n",
    "        if not actual_method_columns_ordered: exit(\"Error: Method mapping file is empty.\")\n",
    "    except FileNotFoundError: exit(f\"Error: Method mapping file not found: {method_mapping_file_path}\")\n",
    "    except json.JSONDecodeError: exit(\"Error: Could not decode JSON from method mapping file.\")\n",
    "\n",
    "    all_models_results = {}\n",
    "    for original_model_name in ORIGINAL_EVALUATION_MODELS:\n",
    "        model_file_tag = get_model_filename_tag(original_model_name) # Generate the tag\n",
    "        processed_file_path = fr'{results_dir}\\output_ranking_comput_{model_file_tag}.csv'\n",
    "        \n",
    "        # Display the original, more readable model name in the report\n",
    "        print(f\"\\n\\n=== Results for Evaluation Model: {original_model_name.upper()} ===\")\n",
    "        if not os.path.exists(processed_file_path):\n",
    "            print(f\"Processed file not found: {processed_file_path}. Skipping this model.\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(processed_file_path, encoding='utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV {processed_file_path}: {e}\"); continue\n",
    "        \n",
    "        current_output_name_map = {f\"output{i+1}_ranking\": name for i, name in enumerate(actual_method_columns_ordered)}\n",
    "        average_list = []\n",
    "        # ... (rest of Script 3 logic for calculating averages remains the same) ...\n",
    "        # It correctly uses current_output_name_map and df\n",
    "        for column_name_in_df, actual_method_name in current_output_name_map.items():\n",
    "            if column_name_in_df in df.columns:\n",
    "                valid_ranks = df[df[column_name_in_df] > 0][column_name_in_df]\n",
    "                avg = valid_ranks.mean() if not valid_ranks.empty else float('inf')\n",
    "                count = len(valid_ranks)\n",
    "                average_list.append({\n",
    "                    \"method_name\": actual_method_name, \"avg_rank\": avg,\n",
    "                    \"evaluated_rows\": count, \"total_rows_in_file\": len(df)\n",
    "                })\n",
    "            else: # Should not happen if Script 2 is correct\n",
    "                 average_list.append({ \n",
    "                    \"method_name\": actual_method_name, \"avg_rank\": float('inf'),\n",
    "                    \"evaluated_rows\": 0, \"total_rows_in_file\": len(df) if 'df' in locals() else 0\n",
    "                })\n",
    "\n",
    "\n",
    "        average_list.sort(key=lambda x: x[\"avg_rank\"])\n",
    "        all_models_results[original_model_name] = average_list # Use original name as key for summary\n",
    "\n",
    "        print(\"\\n--- Average Rankings (lower is better) ---\")\n",
    "        for rank_idx, item in enumerate(average_list, start=1):\n",
    "             print(f\"{rank_idx}. {item['method_name']}: {item['avg_rank']:.2f} (from {item['evaluated_rows']}/{item['total_rows_in_file']})\"\\\n",
    "                   if item['evaluated_rows'] > 0 else f\"{rank_idx}. {item['method_name']}: N/A\")\n",
    "    \n",
    "    print(\"\\n\\n--- Consolidated Summary Across All Evaluation Models ---\")\n",
    "    if all_models_results:\n",
    "        method_summary = {name: {} for name in actual_method_columns_ordered}\n",
    "        for model_name_key, results in all_models_results.items(): # model_name_key is original_model_name\n",
    "            for item in results:\n",
    "                method_summary[item['method_name']][model_name_key] = f\"{item['avg_rank']:.2f} ({item['evaluated_rows']})\" if item['evaluated_rows'] > 0 else \"N/A\"\n",
    "        \n",
    "        header_line = f\"{'Method':<40}\" + \"\".join([f\"{name.upper():<20}\" for name in ORIGINAL_EVALUATION_MODELS])\n",
    "        print(header_line)\n",
    "        print(\"-\" * len(header_line))\n",
    "        for method_name in actual_method_columns_ordered:\n",
    "            row_line = f\"{method_name:<40}\"\n",
    "            for model_key_for_lookup in ORIGINAL_EVALUATION_MODELS:\n",
    "                rank_info = method_summary[method_name].get(model_key_for_lookup, \"N/A\")\n",
    "                row_line += f\"{rank_info:<20}\"\n",
    "            print(row_line)\n",
    "\n",
    "    print(\"\\nScript 3: Reporting complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
